# Meeting 12 - 22 Jan 2021

This meeting started with me giving Jeremy a demonstration of the work I have completed this week. He pointed that he thought the "online only" filter was worded badly and didn't understand what it meant at first. He also thinks that the loading times are slightly too long, making the system frustrating to use at times. Jeremy suggested mentioning performance regressions, such as loading time, in the implementation chapter of my dissertation.

During my initial think aloud evaluations, participants reported that the system was too cluttered and hard to navigate. Jeremy asked if I thought the system was still too cluttered. Potentially could implement A/B testing to test different versions of the UI.

As we were looking through the system, Jeremy noticed that ascii characters, such as &igrave;, aren't being rendered correctly on the frontend. They are just showing the ascii value. I will look into this and fix it next week.

Next, we discussed my test suite which I had added to this week. Jeremy said that it would be nice to see a test case coverage report in my dissertation once my codebase is frozen. He doesn't think that it is necessary to generate a report within my CI pipeline.

We discussed how I should go about conducting an evaluation for my project. We both agreed that I should give my evaluation participants a set of tasks to complete. I will then devise a survey for them to complete. I asked Jeremy how many participants he thought I should ask to complete my evaluation. He said definitely at least 10 but thought around 50 would be a good number. He suggested having between 7 and 15 questions in the evaluation. We agreed that I would prepare a draft evaluation for our meet on Friday 29th January.
